"""
Model Adapters

Adapters for different LLM providers that normalize their APIs to a common interface.
This allows the agent to work with ANY model without changing the core logic.

Supported:
- OpenAI API (GPT-4, GPT-3.5)
- OpenAI-compatible APIs (Cerebras, Groq, etc.)
- Anthropic API (Claude)
- Future: Ollama, HuggingFace, etc.
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional, AsyncGenerator
from uuid import UUID
import logging
import asyncio
from openai import AsyncOpenAI
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

# Optional: Anthropic import (only needed if using Claude)
try:
    from anthropic import AsyncAnthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    AsyncAnthropic = None

logger = logging.getLogger(__name__)


async def get_llm_client(
    user_id: UUID,
    model_name: str,
    db: AsyncSession
) -> AsyncOpenAI:
    """
    Get configured LLM client for a user and model.

    This is the centralized routing function that handles:
    - OpenRouter models: Routes to OpenRouter API with user's stored key
    - Other models: Routes to LiteLLM proxy with user's LiteLLM key

    Args:
        user_id: The user ID
        model_name: The model identifier (e.g., "gpt-4o", "openrouter/anthropic/claude-3.5-sonnet")
        db: Database session

    Returns:
        Configured AsyncOpenAI client ready to use

    Raises:
        ValueError: If user not found or OpenRouter key not configured
    """
    from ..models import User, UserAPIKey
    from ..config import get_settings
    from ..routers.secrets import decode_key

    settings = get_settings()

    # Get user
    result = await db.execute(select(User).where(User.id == user_id))
    user = result.scalar_one_or_none()
    if not user:
        raise ValueError(f"User {user_id} not found")

    # Check if this is an OpenRouter model
    if model_name.startswith("openrouter/"):
        logger.info(f"OpenRouter model detected: {model_name}, fetching user's API key")

        # Get user's OpenRouter API key
        result = await db.execute(
            select(UserAPIKey).where(
                UserAPIKey.user_id == user_id,
                UserAPIKey.provider == "openrouter",
                UserAPIKey.is_active == True
            )
        )
        api_key_record = result.scalar_one_or_none()

        if not api_key_record:
            raise ValueError(
                "OpenRouter model selected but no OpenRouter API key configured. "
                "Please add your OpenRouter API key in Settings."
            )

        # Decode the stored key
        openrouter_key = decode_key(api_key_record.encrypted_value)

        logger.info(f"Using OpenRouter API with user's key for model: {model_name}")

        # Return client configured for OpenRouter
        return AsyncOpenAI(
            api_key=openrouter_key,
            base_url="https://openrouter.ai/api/v1"
        )
    else:
        # Use LiteLLM proxy for system models
        logger.info(f"Using LiteLLM proxy for model: {model_name}")

        if not user.litellm_api_key:
            raise ValueError("User does not have a LiteLLM API key. Please contact support.")

        return AsyncOpenAI(
            api_key=user.litellm_api_key,
            base_url=settings.litellm_api_base
        )


class ModelAdapter(ABC):
    """
    Abstract base class for model adapters.

    All adapters must implement the chat() method which streams the model's text response.
    """

    @abstractmethod
    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> AsyncGenerator[str, None]:
        """
        Send messages to the model and stream text response chunks.

        Args:
            messages: List of message dicts with "role" and "content"
            **kwargs: Model-specific parameters (temperature, max_tokens, etc.)

        Yields:
            Text chunks as they're generated by the model
        """
        pass

    @abstractmethod
    def get_model_name(self) -> str:
        """Get the model name/identifier."""
        pass


class OpenAIAdapter(ModelAdapter):
    """
    Adapter for OpenAI models (GPT-4, GPT-3.5-turbo, etc.)
    Also works with OpenAI-compatible APIs like Cerebras, Groq, Together AI, etc.
    """

    def __init__(
        self,
        model_name: str,
        client: AsyncOpenAI,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None
    ):
        """
        Initialize OpenAI adapter with a pre-configured client.

        Args:
            model_name: Model identifier (e.g., "gpt-4o", "openrouter/anthropic/claude-3.5-sonnet")
            client: Pre-configured AsyncOpenAI client (from get_llm_client())
            temperature: Sampling temperature (0-2)
            max_tokens: Maximum tokens in response
        """
        self.model_name = model_name
        self.client = client
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.is_openrouter = model_name.startswith("openrouter/")

        logger.info(f"OpenAIAdapter initialized - model: {model_name}")

    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> AsyncGenerator[str, None]:
        """
        Send messages to OpenAI API and stream response chunks.

        Args:
            messages: List of message dicts
            **kwargs: Override temperature, max_tokens, etc.

        Yields:
            Text chunks as they're generated by the model
        """
        temperature = kwargs.get("temperature", self.temperature)
        max_tokens = kwargs.get("max_tokens", self.max_tokens)

        # Strip openrouter/ prefix if present (OpenRouter API expects just the model ID)
        model_id = self.model_name.removeprefix("openrouter/") if self.is_openrouter else self.model_name

        request_params = {
            "model": model_id,
            "messages": messages,
            "stream": True  # Enable streaming
        }

        if max_tokens:
            request_params["max_tokens"] = max_tokens

        # Add OpenRouter-specific parameters
        if self.is_openrouter:
            # Add extra_headers for OpenRouter rankings and referrals
            request_params["extra_headers"] = {
                "HTTP-Referer": "https://tesslate.com",  # Your app URL
                "X-Title": "Tesslate Studio"  # Your app name
            }

        try:
            logger.debug(f"Sending streaming request to {self.model_name} with {len(messages)} messages")

            # Create streaming completion
            stream = await self.client.chat.completions.create(**request_params)

            # Stream chunks as they arrive
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if delta.content:
                        yield delta.content

            logger.debug(f"Streaming complete for {self.model_name}")

        except Exception as e:
            logger.error(f"OpenAI API streaming error: {e}", exc_info=True)
            raise RuntimeError(f"Model API error: {str(e)}") from e

    def get_model_name(self) -> str:
        return self.model_name


class AnthropicAdapter(ModelAdapter):
    """
    Adapter for Anthropic's Claude models.
    """

    def __init__(
        self,
        model_name: str,
        api_key: str,
        temperature: float = 0.7,
        max_tokens: int = 4096
    ):
        """
        Initialize Anthropic adapter.

        Args:
            model_name: Model identifier (e.g., "claude-3-5-sonnet-20241022")
            api_key: Anthropic API key
            temperature: Sampling temperature (0-1)
            max_tokens: Maximum tokens in response
        """
        if not ANTHROPIC_AVAILABLE:
            raise ImportError(
                "Anthropic library not installed. Install with: pip install anthropic"
            )

        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.client = AsyncAnthropic(api_key=api_key)

        logger.info(f"AnthropicAdapter initialized - model: {model_name}")

    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> AsyncGenerator[str, None]:
        """
        Send messages to Anthropic API and stream response chunks.

        Note: Anthropic requires system message to be separate from messages list.

        Args:
            messages: List of message dicts
            **kwargs: Override temperature, max_tokens, etc.

        Yields:
            Text chunks as they're generated by the model
        """
        temperature = kwargs.get("temperature", self.temperature)
        max_tokens = kwargs.get("max_tokens", self.max_tokens)

        # Anthropic requires system message to be separate
        system_message = None
        conversation_messages = []

        for msg in messages:
            if msg["role"] == "system":
                system_message = msg["content"]
            else:
                conversation_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })

        try:
            logger.debug(f"Sending streaming request to {self.model_name} with {len(conversation_messages)} messages")

            request_params = {
                "model": self.model_name,
                "messages": conversation_messages,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "stream": True  # Enable streaming
            }

            if system_message:
                request_params["system"] = system_message

            # Stream response chunks
            async with self.client.messages.stream(**request_params) as stream:
                async for text in stream.text_stream:
                    yield text

            logger.debug(f"Streaming complete for {self.model_name}")

        except Exception as e:
            logger.error(f"Anthropic API streaming error: {e}", exc_info=True)
            raise RuntimeError(f"Model API error: {str(e)}") from e

    def get_model_name(self) -> str:
        return self.model_name


async def create_model_adapter(
    model_name: str,
    user_id: UUID,
    db: AsyncSession,
    provider: Optional[str] = None,
    **kwargs
) -> ModelAdapter:
    """
    Factory function to create the appropriate model adapter.

    Uses get_llm_client() to handle model routing (OpenRouter vs LiteLLM).
    Auto-detects provider from model name if not specified.

    Args:
        model_name: Model identifier (e.g., "gpt-4o", "openrouter/anthropic/claude-3.5-sonnet")
        user_id: User ID for fetching API keys
        db: Database session
        provider: Force specific provider ("openai", "anthropic", etc.)
        **kwargs: Additional adapter parameters (temperature, max_tokens, etc.)

    Returns:
        ModelAdapter instance

    Examples:
        # OpenAI GPT-4 (via LiteLLM)
        adapter = await create_model_adapter("gpt-4o", user_id=1, db=db)

        # OpenRouter model (uses user's OpenRouter key)
        adapter = await create_model_adapter("openrouter/anthropic/claude-3.5-sonnet", user_id=1, db=db)

        # Cerebras via LiteLLM
        adapter = await create_model_adapter("cerebras/llama3.1-8b", user_id=1, db=db)
    """
    model_lower = model_name.lower()

    # Auto-detect provider if not specified
    if not provider:
        if "claude" in model_lower or "anthropic" in model_lower:
            # Only use native Anthropic adapter for non-OpenRouter Claude models
            if not model_name.startswith("openrouter/"):
                provider = "anthropic"
            else:
                provider = "openai"  # OpenRouter uses OpenAI-compatible API
        else:
            # Default to OpenAI-compatible
            provider = "openai"

    if provider == "anthropic":
        # Native Anthropic API (not implemented for async client fetching yet)
        # For now, this would require direct API key - not commonly used
        raise NotImplementedError("Native Anthropic adapter not yet updated for centralized routing")
    elif provider == "openai":
        # Get configured client using centralized routing
        client = await get_llm_client(user_id, model_name, db)

        # Create adapter with the configured client
        return OpenAIAdapter(
            model_name=model_name,
            client=client,
            **kwargs
        )
    else:
        raise ValueError(f"Unsupported provider: {provider}")


